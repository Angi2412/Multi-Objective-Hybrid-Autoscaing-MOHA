<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>app.formatting API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>app.formatting</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import logging
import os
import re

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from dotenv import load_dotenv

# init
load_dotenv()
logging.getLogger().setLevel(logging.INFO)


def get_all_data() -&gt; list:
    &#34;&#34;&#34;Gets all metric tables between two dates.
    :return: list of metric data

    Args:

    Returns:

    &#34;&#34;&#34;
    # init
    all_data = list()
    for d in get_directories():
        p_data, c_data, l_data = get_data(d)
        # append to list
        all_data.append([p_data, c_data, l_data])
    return all_data


def get_data(directory: str) -&gt; (pd.DataFrame, pd.DataFrame, pd.DataFrame):
    &#34;&#34;&#34;Gets data from prometheus.
    :return: prometheus data

    Args:
      directory: str) -&gt; (pd.DataFrame: 
      pd.DataFrame: 

    Returns:

    &#34;&#34;&#34;
    # config
    load_dotenv(override=True)
    prometheus_data = None
    prometheus_custom_data = None
    locust_data = None
    # check if folder exists
    data_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;raw&#34;, directory)
    if os.path.exists(data_path):
        # search for prometheus metric files
        logging.info(f&#34;Gets data from {directory}.&#34;)
        for (dir_path, dir_names, filenames) in os.walk(data_path):
            for file in filenames:
                if &#34;metrics&#34; in file and &#34;custom_metrics&#34; not in file:
                    i = int(str(file).split(&#34;_&#34;)[1].rstrip(&#34;.csv&#34;))
                    prometheus_data = get_data_helper(prometheus_data, file, i, directory)
                elif &#34;custom_metrics&#34; in file:
                    j = int(str(file).split(&#34;_&#34;)[2].rstrip(&#34;.csv&#34;))
                    print(j)
                    prometheus_custom_data = get_data_helper(prometheus_custom_data, file, j, directory)
                elif &#34;locust&#34; in file and &#34;stats&#34; in file and &#34;history&#34; not in file:
                    l = int(str(file).split(&#34;_&#34;)[2].rstrip(&#34;.csv&#34;))
                    locust_data = get_data_helper(locust_data, file, l, directory)
    return prometheus_data, prometheus_custom_data, locust_data


def get_data_helper(data: pd.DataFrame, file: str, iteration: int, directory: str) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Connects two dataframes.

    Args:
      data: given dataframe
      file: data frame in file
      iteration: number of iteration
      directory: date
      data: pd.DataFrame: 
      file: str: 
      iteration: int: 
      directory: str: 

    Returns:
      connected data frame

    &#34;&#34;&#34;
    data_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;raw&#34;, directory)
    # concat metrics
    tmp_data = pd.read_csv(filepath_or_buffer=os.path.join(data_path, file), delimiter=&#39;,&#39;)
    tmp_data.insert(0, &#39;Iteration&#39;, iteration)
    if data is None:
        data = tmp_data
    else:
        data = pd.concat([data, tmp_data])
    return data


def get_directories() -&gt; list:
    &#34;&#34;&#34;Gets all directory names between the first and last data date.
    :return: list of directory names

    Args:

    Returns:

    &#34;&#34;&#34;
    load_dotenv()
    first_date = int(str(os.getenv(&#34;FIRST_DATA&#34;)).replace(&#39;-&#39;, &#34;&#34;).strip())
    last_date = int(str(os.getenv(&#34;LAST_DATA&#34;)).replace(&#39;-&#39;, &#34;&#34;).strip())
    base_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;raw&#34;)
    dirs = list()
    # get data from each run
    for (dir_path, dir_names, filenames) in os.walk(base_path):
        for c_dir in dir_names:
            if &#34;dataset&#34; not in c_dir:
                c_date = int(str(c_dir).replace(&#39;-&#39;, &#34;&#34;).strip())
                if last_date &gt;= c_date &gt;= first_date:
                    dirs.append(c_dir)
    return dirs


def get_filtered_data(directory: str) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Returns a data frame of a given filtered data.

    Args:
      directory: date
      directory: str: 

    Returns:
      data frame of filtered data

    &#34;&#34;&#34;
    base_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;filtered&#34;)
    for (dir_path, dir_names, filenames) in os.walk(base_path):
        for c_file in filenames:
            if directory in c_file:
                df = pd.read_csv(os.path.join(base_path, c_file))
                return df


def get_all_filtered_data() -&gt; list:
    &#34;&#34;&#34;Reads all filtered data between two dates.
    :return: list of filtered data

    Args:

    Returns:

    &#34;&#34;&#34;
    load_dotenv()
    first_date = int(str(os.getenv(&#34;FIRST_DATA&#34;)).replace(&#39;-&#39;, &#34;&#34;).strip())
    last_date = int(str(os.getenv(&#34;LAST_DATA&#34;)).replace(&#39;-&#39;, &#34;&#34;).strip())
    base_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;filtered&#34;)
    files = list()
    # get data from each run
    for (dir_path, dir_names, filenames) in os.walk(base_path):
        for c_file in filenames:
            if str(c_file).endswith(&#34;.csv&#34;):
                c_date = int(str(c_file).replace(&#39;-&#39;, &#34;&#34;).replace(&#34;.csv&#34;, &#34;&#34;).strip())
                if last_date &gt;= c_date &gt;= first_date:
                    files.append(pd.read_csv(os.path.join(base_path, c_file)))
    return files


def filter_all_data() -&gt; None:
    &#34;&#34;&#34;Filters all data between two dates.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    # init
    i = 1
    dirs = get_directories()
    for d in dirs:
        # filter data in directory
        logging.info(f&#34;Filtering data: {d} {i}/{len(dirs)}&#34;)
        filter_data(d)
        i = i + 1


def get_variation_matrix(directory: str) -&gt; np.array:
    &#34;&#34;&#34;Reads all variation matrices of a directory and puts them in a list.

    Args:
      directory: current directory
      directory: str: 

    Returns:
      variation matrix

    &#34;&#34;&#34;
    dir_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;raw&#34;, directory)
    # find variation files
    for (dir_path, dir_names, filenames) in os.walk(dir_path):
        for file in filenames:
            if &#34;variation&#34; in file:
                # filter name
                name = str(file).split(&#34;-&#34;)[1].split(&#34;_&#34;)[0]
                # read variation file
                file_path = os.path.join(dir_path, file)
                res = pd.read_csv(filepath_or_buffer=file_path, delimiter=&#39;,&#39;)
                # edit table
                res.insert(0, &#39;pod&#39;, name)
                res.rename(columns={&#34;Unnamed: 0&#34;: &#34;Iteration&#34;}, inplace=True)
                res.reset_index()
                return res


def filter_data(directory: str) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Filters data from prometheus.
    :return: filtered data

    Args:
      directory: str: 

    Returns:

    &#34;&#34;&#34;
    normal, custom, locust = get_data(directory)
    # filter for namespace
    filtered_data = pd.concat(objs=[normal[normal.namespace.eq(os.getenv(&#34;NAMESPACE&#34;))]])
    # read variation matrices
    variation = get_variation_matrix(directory)
    # filter for pod name
    filtered_data[&#34;pod&#34;] = filtered_data[&#34;pod&#34;].str.split(&#34;-&#34;, n=2).str[1]
    custom[&#34;pod&#34;] = custom[&#34;pod&#34;].str.split(&#34;-&#34;, n=2).str[1]
    # count data points per iteration
    filtered_data[&#39;datapoint&#39;] = filtered_data.groupby([&#34;Iteration&#34;]).cumcount() + 1
    custom[&#39;datapoint&#39;] = custom.groupby([&#34;Iteration&#34;]).cumcount() + 1
    custom[&#39;pod&#39;].fillna(&#34;webui&#34;, inplace=True)
    # create pivot tables
    filtered_data = pd.pivot_table(filtered_data, index=[&#34;Iteration&#34;, &#34;pod&#34;, &#34;datapoint&#34;], columns=[&#34;__name__&#34;],
                                   values=&#34;value&#34;).reset_index()
    filtered_custom_data = pd.pivot_table(custom, index=[&#34;Iteration&#34;, &#34;pod&#34;, &#34;datapoint&#34;], columns=[&#34;metric&#34;],
                                          values=&#34;value&#34;).reset_index()
    # calculate median values
    filtered_data = filtered_data.groupby([&#34;Iteration&#34;, &#34;pod&#34;]).mean().reset_index()
    filtered_custom_data = filtered_custom_data.groupby([&#34;Iteration&#34;, &#34;pod&#34;]).mean().reset_index()
    filtered_custom_data.rename(columns={&#34;rps&#34;: &#34;average rps&#34;}, inplace=True)
    # outliers
    filtered_custom_data[&#34;median_latency&#34;] = np.where(
        filtered_custom_data[&#34;median_latency&#34;] &lt; filtered_custom_data[&#34;median_latency&#34;].quantile(0.10),
        filtered_custom_data[&#34;median_latency&#34;].quantile(0.10),
        filtered_custom_data[&#39;median_latency&#39;])
    filtered_custom_data[&#34;median_latency&#34;] = np.where(
        filtered_custom_data[&#34;median_latency&#34;] &gt; filtered_custom_data[&#34;median_latency&#34;].quantile(0.90),
        filtered_custom_data[&#34;median_latency&#34;].quantile(0.90),
        filtered_custom_data[&#39;median_latency&#39;])
    # merge all tables
    res_data = pd.merge(filtered_data, filtered_custom_data, how=&#39;left&#39;, on=[&#34;Iteration&#34;, &#34;pod&#34;])
    res_data = pd.merge(res_data, variation, how=&#39;left&#39;, on=[&#34;Iteration&#34;, &#34;pod&#34;])
    # erase stuff
    res_data.drop(columns=[&#34;kube_deployment_spec_replicas&#34;, &#34;kube_pod_container_resource_limits_cpu_cores&#34;,
                           &#34;kube_pod_container_resource_limits_memory_bytes&#34;,
                           &#34;kube_pod_container_resource_requests_cpu_cores&#34;,
                           &#34;kube_pod_container_resource_requests_memory_bytes&#34;,
                           &#34;datapoint_x&#34;, &#34;datapoint_y&#34;], inplace=True)
    res_data.rename(
        columns={&#34;cpu&#34;: &#34;cpu usage&#34;, &#34;memory&#34;: &#34;memory usage&#34;, &#34;CPU&#34;: &#34;cpu limit&#34;, &#34;Memory&#34;: &#34;memory limit&#34;,
                 &#34;Pods&#34;: &#34;number of pods&#34;, &#34;container_cpu_cfs_throttled_seconds_total&#34;: &#34;cpu throttled total&#34;,
                 &#34;response_time&#34;: &#34;average response time&#34;, &#34;median_latency&#34;: &#34;median latency&#34;},
        inplace=True)
    # ratio
    res_data[&#34;rps delta&#34;] = (res_data[&#34;average rps&#34;] - res_data[&#34;RPS&#34;]) / res_data[&#34;RPS&#34;]
    res_data[&#34;ratio response time&#34;] = res_data[&#34;median latency&#34;] * (1 - res_data[&#34;rps delta&#34;])
    res_data[&#34;ratio cpu usage&#34;] = res_data[&#34;cpu usage&#34;] * (1 - res_data[&#34;rps delta&#34;])
    res_data[&#34;ratio memory usage&#34;] = res_data[&#34;memory usage&#34;] * (1 - res_data[&#34;rps delta&#34;])
    # filter for webui pod
    res_data = res_data.loc[(res_data[&#39;pod&#39;] == &#34;webui&#34;)]
    res_data.reset_index(inplace=True)
    res_data.drop(columns=[&#34;index&#34;], inplace=True)
    # save
    save_data(res_data, directory, &#34;filtered&#34;)
    return res_data


def save_data(data: pd.DataFrame, directory: str, mode: str) -&gt; None:
    &#34;&#34;&#34;Saves a given data frame in a given folder.

    Args:
      data: data frame
      directory: name of file
      mode: name of directory
      data: pd.DataFrame: 
      directory: str: 
      mode: str: 

    Returns:
      None

    &#34;&#34;&#34;
    save_path = os.path.join(os.getcwd(), &#34;data&#34;, mode, f&#34;{directory}.csv&#34;)
    if not os.path.exists(save_path):
        data.to_csv(path_or_buf=save_path)
    else:
        logging.warning(&#34;Filtered data already exists.&#34;)


def plot_filtered_data(data: pd.DataFrame, name: str) -&gt; None:
    &#34;&#34;&#34;Plots a given metric from filtered data from prometheus.
    :return: None

    Args:
      data: pd.DataFrame: 
      name: str: 

    Returns:

    &#34;&#34;&#34;
    # create directory
    dir_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;plots&#34;, f&#34;{name}&#34;)
    os.mkdir(dir_path)
    # init x- and y-axis
    x_axis = [&#34;cpu limit&#34;, &#34;memory limit&#34;, &#34;number of pods&#34;]
    y_axis = [&#34;ratio response time&#34;, &#34;ratio cpu usage&#34;,
              &#34;ratio memory usage&#34;]
    # functions
    functions = [pd.DataFrame.min, pd.DataFrame.median, pd.DataFrame.max]
    # create and save plots
    plot = None
    for fn in functions:
        for y in y_axis:
            for x in x_axis:
                if x == &#34;number of pods&#34;:
                    data_pods = data.loc[(data[&#39;memory limit&#39;] == fn(data[&#39;memory limit&#39;])) &amp; (
                            data[&#39;cpu limit&#39;] == fn(data[&#39;cpu limit&#39;]))]
                    plot = sns.lineplot(data=data_pods, x=x, y=y)
                elif x == &#34;memory limit&#34;:
                    data_memory = data.loc[(data[&#39;cpu limit&#39;] == fn(data[&#39;cpu limit&#39;]))]
                    plot = sns.lineplot(data=data_memory, x=x, y=y, hue=&#34;number of pods&#34;)
                elif x == &#34;cpu limit&#34;:
                    data_cpu = data.loc[(data[&#39;memory limit&#39;] == fn(data[&#39;memory limit&#39;]))]
                    plot = sns.lineplot(data=data_cpu, x=x, y=y, hue=&#34;number of pods&#34;)
                name = f&#34;{x}_{y}_{fn.__name__}.png&#34;
                plot.figure.savefig(os.path.join(dir_path, name))
                plot.figure.clf()
    # Requests per second
    fig, ax = plt.subplots()
    ax.plot(data[&#34;Iteration&#34;], data[&#34;average rps&#34;])
    ax.plot(data[&#34;Iteration&#34;], data[&#34;RPS&#34;])
    fig.savefig(os.path.join(dir_path, &#34;rps.png&#34;))
    fig.clf()


def format_for_extra_p() -&gt; None:
    &#34;&#34;&#34;Formats a given benchmark for extra-p.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    # init
    load_dotenv()
    save_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;formatted&#34;, os.getenv(&#34;LAST_DATA&#34;))
    # create directory if not existing
    if not os.path.exists(save_path):
        os.mkdir(save_path)
    # get variation matrix
    variation = get_variation_matrix(os.getenv(&#34;LAST_DATA&#34;))
    m_max = variation[&#34;Memory&#34;].nunique()
    # parameter and metrics
    parameter = [&#34;cpu limit&#34;, &#34;memory limit&#34;, &#34;number of pods&#34;, &#34;rps&#34;]
    targets = [&#34;average response time&#34;, &#34;cpu usage&#34;, &#34;memory usage&#34;]
    # get all filtered data
    filtered_data = list()
    for f in get_all_filtered_data():
        filtered_data.append(f)
    # write in txt file
    for metric in targets:
        m_name = (re.sub(&#39;[^a-zA-Z0-9 _]&#39;, &#39;&#39;, metric)).rstrip().replace(&#39; &#39;, &#39;_&#39;).lower()
        with open(os.path.join(save_path, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}_{m_name}_extra-p.txt&#34;), &#34;x&#34;) as file:
            # write parameters
            for par in parameter:
                file.write(f&#34;PARAMETER {(re.sub(&#39;[^a-zA-Z0-9 _]&#39;, &#39;&#39;, par)).rstrip().replace(&#39; &#39;, &#39;_&#39;).lower()}\n&#34;)
            file.write(&#34;\n&#34;)
            # write coordinates
            # for every iteration
            for i, v in enumerate(variation.index):
                if i % m_max == 0:
                    file.write(&#34;\n&#34;)
                    file.write(&#34;POINTS &#34;)
                row = variation.iloc[[i]]
                file.write(
                    f&#34;( {row.iloc[0][&#39;CPU&#39;]} {row.iloc[0][&#39;Memory&#39;]} {row.iloc[0][&#39;Pods&#39;]} {row.iloc[0][&#39;RPS&#39;]} ) &#34;)
            file.write(&#34;\n&#34;)
            file.write(&#34;\n&#34;)
            file.write(f&#34;REGION {os.getenv(&#39;APP_NAME&#39;)}\n&#34;)
            file.write(f&#34;METRIC {m_name}\n&#34;)
            file.write(&#34;\n&#34;)
            # write data
            # for every datapoint
            for i in range(0, (filtered_data[0].index.max() + 1)):
                logging.info(f&#34;format data: {i + 1}/{(filtered_data[0].index.max() + 1)}&#34;)
                file.write(&#34;DATA &#34;)
                # for test purposes
                for f in filtered_data:
                    x = f.loc[f.index == i, metric].iloc[0]
                    file.write(f&#34;{x} &#34;)
                file.write(&#34;\n&#34;)


def correlation_coefficient_matrix() -&gt; None:
    &#34;&#34;&#34;Calculates and plots the correlation coefficient matrix for a given dataframe.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    dir_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;correlation&#34;, os.getenv(&#34;LAST_DATA&#34;))
    if not os.path.exists(dir_path):
        os.mkdir(dir_path)
    df = pd.read_csv(os.path.join(os.getcwd(), &#34;data&#34;, &#34;combined&#34;, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}.csv&#34;))
    df = df[
        [&#34;cpu limit&#34;, &#34;memory limit&#34;, &#34;number of pods&#34;, &#34;ratio response time&#34;, &#34;ratio cpu usage&#34;,
         &#34;ratio memory usage&#34;]]
    df.dropna()
    corr = df.corr(method=&#34;pearson&#34;)
    save_data(corr, os.getenv(&#34;LAST_DATA&#34;), os.path.join(&#34;correlation&#34;, os.getenv(&#34;LAST_DATA&#34;)))
    # plot correlation
    # Generate a mask for the upper triangle
    mask = np.triu(np.ones_like(corr, dtype=bool))
    # Set up the matplotlib figure
    f, ax = plt.subplots(figsize=(11, 9))
    # Generate a custom diverging colormap
    cmap = sns.color_palette(&#34;coolwarm&#34;, as_cmap=True)
    # Draw the heatmap with the mask and correct aspect ratio
    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0, square=True, linewidths=.5,
                cbar_kws={&#34;shrink&#34;: .5},
                annot=True, fmt=&#34;.2f&#34;)
    f.savefig(os.path.join(dir_path, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}.png&#34;), bbox_inches=&#39;tight&#39;)
    plt.show()


def combine_runs() -&gt; None:
    &#34;&#34;&#34;Combines data from all runs.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    data = get_directories()
    data_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;filtered&#34;)
    tmp = list()
    for i, file in enumerate(data):
        tmp_data = pd.read_csv(filepath_or_buffer=os.path.join(data_path, f&#34;{file}.csv&#34;), delimiter=&#39;,&#39;)
        tmp_data.insert(0, &#39;run&#39;, i + 1)
        tmp_data = tmp_data.loc[(tmp_data[&#39;pod&#39;] == &#34;webui&#34;)]
        tmp.append(tmp_data)
    result = pd.concat(tmp)
    save_data(result, os.getenv(&#34;LAST_DATA&#34;), &#34;combined&#34;)


def combine_data(data: list, name: str) -&gt; None:
    &#34;&#34;&#34;Combines data from all runs.
    :return: None

    Args:
      data: list: 
      name: str: 

    Returns:

    &#34;&#34;&#34;
    data_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;filtered&#34;)
    tmp = list()
    for i, file in enumerate(data):
        tmp_data = pd.read_csv(filepath_or_buffer=os.path.join(data_path, f&#34;{file}.csv&#34;), delimiter=&#39;,&#39;)
        tmp_data.insert(0, &#39;run&#39;, i + 1)
        tmp_data = tmp_data.loc[(tmp_data[&#39;pod&#39;] == &#34;webui&#34;)]
        tmp.append(tmp_data)
    result = pd.concat(tmp)
    save_data(result, name, &#34;combined&#34;)


def filter_run() -&gt; None:
    &#34;&#34;&#34;Filters all data from a run.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;combined&#34;, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}.csv&#34;)
    data = pd.read_csv(path, delimiter=&#34;,&#34;)
    data = data.groupby([&#34;Iteration&#34;, &#34;pod&#34;]).median()
    save_data(data, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}_median&#34;, &#34;combined&#34;)


def plot_run() -&gt; None:
    &#34;&#34;&#34;Plots all data from a run.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    # plot combined run
    path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;combined&#34;, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}.csv&#34;)
    data = pd.read_csv(path, delimiter=&#34;,&#34;)
    plot_filtered_data(data, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}_combined&#34;)
    # plot mean run
    path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;combined&#34;, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}_median.csv&#34;)
    data = pd.read_csv(path, delimiter=&#34;,&#34;)
    plot_filtered_data(data, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}_combined_median&#34;)


def plot_all_data():
    &#34;&#34;&#34;Plots data from all runs.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    directory = os.path.join(os.getcwd(), &#34;data&#34;, &#34;plots&#34;, os.getenv(&#34;LAST_DATA&#34;))
    # creates folder if does not exist
    if not os.path.exists(directory):
        os.mkdir(directory)
    for i, file in enumerate(get_all_filtered_data()):
        plot_filtered_data(file, os.path.join(os.getenv(&#34;LAST_DATA&#34;), str(i)))


def plot_targets_4d(data: pd.DataFrame, name: str) -&gt; None:
    &#34;&#34;&#34;Plot all parameters and each target in a 4D plot.

    Args:
      data: dataset
      name: save name
      data: pd.DataFrame: 
      name: str: 

    Returns:
      None

    &#34;&#34;&#34;
    targets = [&#34;average response time&#34;, &#34;cpu usage&#34;, &#34;memory usage&#34;]
    for t in targets:
        fig = plt.figure()
        ax = fig.add_subplot(111, projection=&#39;3d&#39;)

        x = data[&#34;cpu limit&#34;]
        y = data[&#34;memory limit&#34;]
        z = data[&#34;number of pods&#34;]
        c = data[t]

        img = ax.scatter(x, y, z, c=c, cmap=plt.jet())
        fig.colorbar(img)
        plt.show()
        # save figure
        save_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;plots&#34;, name)
        if not os.path.exists(save_path):
            os.mkdir(save_path)
        fig.savefig(os.path.join(save_path, f&#34;{t}_3D.png&#34;))
        fig.clf()


def get_combined_data() -&gt; pd.DataFrame:
    &#34;&#34;&#34;

    Args:

    Returns:
      :return: combined data

    &#34;&#34;&#34;
    # get last combined run
    path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;combined&#34;, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}.csv&#34;)
    data = pd.read_csv(path, delimiter=&#34;,&#34;)
    return data


def histogram() -&gt; None:
    &#34;&#34;&#34;Creates histogram of the average response time.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    sns.set_style(&#34;whitegrid&#34;)
    data = get_combined_data()
    ax = sns.histplot(data, x=&#34;average response time&#34;,
                      bins=50, binrange=(250, 5000))
    ax.set_xlabel(&#34;Average Response Time [ms]&#34;)
    plt.show()


def boxplot() -&gt; None:
    &#34;&#34;&#34;Creates boxplot from the CPU- and memory usage.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    data = get_combined_data()
    ax = sns.boxplot(data=data[[&#34;cpu usage&#34;, &#34;memory usage&#34;]])
    ax.set_ylabel(&#34;Usage [%]&#34;)
    plt.show()


def stats(column: str) -&gt; None:
    &#34;&#34;&#34;Prints stats of a given column of the combined data.

    Args:
      column: metric to be described
      column: str: 

    Returns:
      None

    &#34;&#34;&#34;
    data = get_combined_data()
    print(data[column].describe())


def process_run() -&gt; None:
    &#34;&#34;&#34;Processes one single run.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    filter_data(os.getenv(&#34;LAST_DATA&#34;))
    plot_filtered_data(get_filtered_data(os.getenv(&#34;LAST_DATA&#34;)), os.getenv(&#34;LAST_DATA&#34;))


def process_all_runs() -&gt; None:
    &#34;&#34;&#34;Processes all runs between start- and last data.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    filter_all_data()
    plot_all_data()
    combine_runs()
    filter_run()
    plot_run()
    format_for_extra_p()
    correlation_coefficient_matrix()


def formatting_evaluation(date: str) -&gt; (pd.DataFrame, pd.DataFrame):
    &#34;&#34;&#34;Formats evaluation data.

    Args:
      date: date of evaluation raw folder
      date: str) -&gt; (pd.DataFrame: 
      pd.DataFrame: 

    Returns:
      custom and normal filtered data

    &#34;&#34;&#34;
    # get data
    dir_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;raw&#34;, f&#34;{date}&#34;)
    custom = pd.read_csv(os.path.join(dir_path, &#34;custom_metrics_0.csv&#34;), delimiter=&#39;,&#39;)
    normal = pd.read_csv(os.path.join(dir_path, &#34;metrics_0.csv&#34;), delimiter=&#39;,&#39;)
    if os.path.exists(os.path.join(dir_path, &#34;custom_metrics_0.csv&#34;)) and os.path.exists(
            os.path.join(dir_path, &#34;metrics_0.csv&#34;)):
        # filter for namespace
        filtered_data = pd.concat(objs=[normal[normal.namespace.eq(os.getenv(&#34;NAMESPACE&#34;))]])
        # filter for pod name
        filtered_data[&#34;pod&#34;] = filtered_data[&#34;pod&#34;].str.split(&#34;-&#34;, n=2).str[1]
        custom[&#34;pod&#34;] = custom[&#34;pod&#34;].str.split(&#34;-&#34;, n=2).str[1]
        custom[&#39;pod&#39;].fillna(&#34;webui&#34;, inplace=True)
        # timestamps in seconds
        filtered_data[&#34;timestamp&#34;] = pd.to_datetime(filtered_data[&#34;timestamp&#34;], unit=&#39;s&#39;, origin=&#39;unix&#39;)
        filtered_data[&#34;timestamp&#34;] = (filtered_data[&#34;timestamp&#34;] - filtered_data[&#34;timestamp&#34;].min())
        custom[&#34;timestamp&#34;] = pd.to_datetime(custom[&#34;timestamp&#34;], unit=&#39;s&#39;, origin=&#39;unix&#39;)
        custom[&#34;timestamp&#34;] = (custom[&#34;timestamp&#34;] - custom[&#34;timestamp&#34;].min())
        filtered_data[&#34;timestamp&#34;] = (filtered_data.timestamp / np.timedelta64(1, &#39;m&#39;))
        custom[&#34;timestamp&#34;] = (custom.timestamp / np.timedelta64(1, &#39;m&#39;))
        # filter for pod
        filtered_data_a = filtered_data[filtered_data[&#39;deployment&#39;] == &#34;teastore-webui&#34;]
        filtered_data_b = filtered_data[filtered_data[&#39;pod&#39;] == &#34;webui&#34;]
        filtered_data = pd.concat([filtered_data_a, filtered_data_b])
        custom = custom[(custom[&#39;pod&#39;] == &#34;webui&#34;)]
        # pivot
        filtered_data = pd.pivot_table(filtered_data, index=[&#34;timestamp&#34;], columns=[&#34;__name__&#34;],
                                       values=&#34;value&#34;).reset_index()
        filtered_custom_data = pd.pivot_table(custom, index=[&#34;timestamp&#34;], columns=[&#34;metric&#34;],
                                              values=&#34;value&#34;).reset_index()
        filtered_custom_data.set_index(&#34;timestamp&#34;)
        # fix units
        filtered_data[&#34;kube_pod_container_resource_limits_cpu_cores&#34;] = filtered_data[
                                                                            &#34;kube_pod_container_resource_limits_cpu_cores&#34;] * 1000
        filtered_data[&#34;kube_pod_container_resource_limits_memory_bytes&#34;] = filtered_data[
                                                                               &#34;kube_pod_container_resource_limits_memory_bytes&#34;] / 1048576
        filtered_data.to_csv(os.path.join(dir_path, &#34;filtered.csv&#34;))
        filtered_custom_data.to_csv(os.path.join(dir_path, &#34;filtered_custom.csv&#34;))
        return filtered_data, filtered_custom_data, dir_path
    else:
        logging.warning(f&#34;Metric files do not exist in folder: {date}&#34;)
        return


def plot_evaluation(date: str) -&gt; None:
    &#34;&#34;&#34;Plots a given evaluation run.

    Args:
      date: name of folder
      date: str: 

    Returns:
      None

    &#34;&#34;&#34;
    # format raw data
    n, c, dir_path = formatting_evaluation(date)

    # kube metrics
    nfig, (nax1, nax2, nax3) = plt.subplots(nrows=3, ncols=1)
    # cpu limit
    sns.lineplot(data=n, x=&#34;timestamp&#34;, y=&#34;kube_pod_container_resource_limits_cpu_cores&#34;, ax=nax1)
    nax1.set_ylabel(&#34;CPU limit [m]&#34;)
    # memory limit
    sns.lineplot(data=n, x=&#34;timestamp&#34;, y=&#34;kube_pod_container_resource_limits_memory_bytes&#34;, ax=nax2)
    nax2.set_ylabel(&#34;Memory limit [Mi]&#34;)
    # number of pods
    sns.lineplot(data=n, x=&#34;timestamp&#34;, y=&#34;kube_deployment_spec_replicas&#34;, ax=nax3)
    nax3.set_ylabel(&#34;Number of pods&#34;)
    nfig.savefig(os.path.join(dir_path, &#34;parameters.png&#34;))
    nfig.clf()

    # custom metrics
    cfig, (cax1, cax2, cax3) = plt.subplots(nrows=3, ncols=1)
    # average response time
    sns.lineplot(data=c, x=&#34;timestamp&#34;, y=&#34;response_time&#34;, ax=cax1)
    cax1.set_ylabel(&#34;Response time [ms]&#34;)
    # cpu
    sns.lineplot(data=c, x=&#34;timestamp&#34;, y=&#34;cpu&#34;, ax=cax2)
    cax2.set_ylabel(&#34;CPU usage [%]&#34;)
    # memory
    sns.lineplot(data=c, x=&#34;timestamp&#34;, y=&#34;memory&#34;, ax=cax3)
    cax3.set_ylabel(&#34;Memory usage [%]&#34;)
    cfig.savefig(os.path.join(dir_path, &#34;targets.png&#34;))
    cfig.clf()

    description = {&#34;response_time&#34;: &#34;Average response time [ms]&#34;, &#34;cpu&#34;: &#34;CPU usage [%]&#34;, &#34;memory&#34;: &#34;Memory usage [%]&#34;}
    for t in [&#34;response_time&#34;, &#34;cpu&#34;, &#34;memory&#34;]:
        ax = sns.histplot(c, x=t)
        ax.set_xlabel(description[t])
        fig = ax.get_figure()
        fig.savefig(os.path.join(dir_path, f&#34;{t}_histogram.png&#34;))
        fig.clf()
    # rps
    ax = sns.lineplot(data=c, x=&#34;timestamp&#34;, y=&#34;rps&#34;)
    ax.set_xlabel(&#34;Minutes&#34;)
    ax.set_ylabel(&#34;Requests per second&#34;)
    ax.figure.savefig(os.path.join(dir_path, &#34;rps.png&#34;))
    cfig.clf()
    calc_eval_metrics(c, n, dir_path)


def calc_eval_metrics(c: pd.DataFrame, n: pd.DataFrame, dir_path: str) -&gt; None:
    &#34;&#34;&#34;Calculated evaluation metrics.

    Args:
      c: custom data
      n: normal data
      dir_path: save path
      c: pd.DataFrame: 
      n: pd.DataFrame: 
      dir_path: str: 

    Returns:
      None

    &#34;&#34;&#34;
    # init thresholds
    r = int(os.getenv(&#34;TARGET_RESPONSE&#34;))
    min_usage = int(os.getenv(&#34;MIN_USAGE&#34;))
    max_usage = int(os.getenv(&#34;MAX_USAGE&#34;))
    # validate
    response_time_exceeded = c[c.response_time &gt; r].count()
    response_time_valid = c[c.response_time &lt;= r].count()
    cpu_in_threshold = c[(min_usage &lt;= c.cpu) &amp; (c.cpu &lt;= max_usage)].count()
    cpu_out_threshold = c[(c.cpu &lt; min_usage) | (c.cpu &gt; max_usage)].count()
    memory_in_threshold = c[(min_usage &lt;= c.memory) &amp; (c.memory &lt;= max_usage)].count()
    memory_out_threshold = c[(c.memory &lt; min_usage) | (c.memory &gt; max_usage)].count()
    # create dataframe
    result = pd.DataFrame()
    result[&#34;response time exceeded&#34;] = response_time_exceeded
    result[&#34;response time valid&#34;] = response_time_valid
    result[&#34;cpu in threshold&#34;] = cpu_in_threshold
    result[&#34;cpu out of threshold&#34;] = cpu_out_threshold
    result[&#34;memory in threshold&#34;] = memory_in_threshold
    result[&#34;memory out of threshold&#34;] = memory_out_threshold
    result[&#34;average cpu limit&#34;] = n[&#34;kube_pod_container_resource_limits_cpu_cores&#34;].mean()
    result[&#34;average memory limit&#34;] = n[&#34;kube_pod_container_resource_limits_memory_bytes&#34;].mean()
    result[&#34;average pods&#34;] = n[&#34;kube_deployment_spec_replicas&#34;].mean()
    result[&#34;metric 50/50&#34;] = 0.5 * response_time_exceeded + 0.5 * (cpu_out_threshold + memory_out_threshold) * n[
        &#34;kube_deployment_spec_replicas&#34;].mean()
    result[&#34;median response time&#34;] = c[&#34;response_time&#34;].median()
    result[&#34;average cpu usage&#34;] = c[&#34;cpu&#34;].mean()
    result[&#34;average memory usage&#34;] = c[&#34;memory&#34;].mean()
    result.to_csv(os.path.join(dir_path, &#34;results.csv&#34;))


def plot_all_evaluation() -&gt; None:
    &#34;&#34;&#34;Plots all evaluations in the raw folder.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    raw_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;raw&#34;)
    for (dir_path, dir_names, filenames) in os.walk(raw_path):
        for d in dir_names:
            plot_evaluation(d)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="app.formatting.boxplot"><code class="name flex">
<span>def <span class="ident">boxplot</span></span>(<span>) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Creates boxplot from the CPU- and memory usage.
:return: None</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def boxplot() -&gt; None:
    &#34;&#34;&#34;Creates boxplot from the CPU- and memory usage.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    data = get_combined_data()
    ax = sns.boxplot(data=data[[&#34;cpu usage&#34;, &#34;memory usage&#34;]])
    ax.set_ylabel(&#34;Usage [%]&#34;)
    plt.show()</code></pre>
</details>
</dd>
<dt id="app.formatting.calc_eval_metrics"><code class="name flex">
<span>def <span class="ident">calc_eval_metrics</span></span>(<span>c: pandas.core.frame.DataFrame, n: pandas.core.frame.DataFrame, dir_path: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Calculated evaluation metrics.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>c</code></strong></dt>
<dd>custom data</dd>
<dt><strong><code>n</code></strong></dt>
<dd>normal data</dd>
<dt><strong><code>dir_path</code></strong></dt>
<dd>save path</dd>
<dt><strong><code>c</code></strong></dt>
<dd>pd.DataFrame: </dd>
<dt><strong><code>n</code></strong></dt>
<dd>pd.DataFrame: </dd>
<dt><strong><code>dir_path</code></strong></dt>
<dd>str: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_eval_metrics(c: pd.DataFrame, n: pd.DataFrame, dir_path: str) -&gt; None:
    &#34;&#34;&#34;Calculated evaluation metrics.

    Args:
      c: custom data
      n: normal data
      dir_path: save path
      c: pd.DataFrame: 
      n: pd.DataFrame: 
      dir_path: str: 

    Returns:
      None

    &#34;&#34;&#34;
    # init thresholds
    r = int(os.getenv(&#34;TARGET_RESPONSE&#34;))
    min_usage = int(os.getenv(&#34;MIN_USAGE&#34;))
    max_usage = int(os.getenv(&#34;MAX_USAGE&#34;))
    # validate
    response_time_exceeded = c[c.response_time &gt; r].count()
    response_time_valid = c[c.response_time &lt;= r].count()
    cpu_in_threshold = c[(min_usage &lt;= c.cpu) &amp; (c.cpu &lt;= max_usage)].count()
    cpu_out_threshold = c[(c.cpu &lt; min_usage) | (c.cpu &gt; max_usage)].count()
    memory_in_threshold = c[(min_usage &lt;= c.memory) &amp; (c.memory &lt;= max_usage)].count()
    memory_out_threshold = c[(c.memory &lt; min_usage) | (c.memory &gt; max_usage)].count()
    # create dataframe
    result = pd.DataFrame()
    result[&#34;response time exceeded&#34;] = response_time_exceeded
    result[&#34;response time valid&#34;] = response_time_valid
    result[&#34;cpu in threshold&#34;] = cpu_in_threshold
    result[&#34;cpu out of threshold&#34;] = cpu_out_threshold
    result[&#34;memory in threshold&#34;] = memory_in_threshold
    result[&#34;memory out of threshold&#34;] = memory_out_threshold
    result[&#34;average cpu limit&#34;] = n[&#34;kube_pod_container_resource_limits_cpu_cores&#34;].mean()
    result[&#34;average memory limit&#34;] = n[&#34;kube_pod_container_resource_limits_memory_bytes&#34;].mean()
    result[&#34;average pods&#34;] = n[&#34;kube_deployment_spec_replicas&#34;].mean()
    result[&#34;metric 50/50&#34;] = 0.5 * response_time_exceeded + 0.5 * (cpu_out_threshold + memory_out_threshold) * n[
        &#34;kube_deployment_spec_replicas&#34;].mean()
    result[&#34;median response time&#34;] = c[&#34;response_time&#34;].median()
    result[&#34;average cpu usage&#34;] = c[&#34;cpu&#34;].mean()
    result[&#34;average memory usage&#34;] = c[&#34;memory&#34;].mean()
    result.to_csv(os.path.join(dir_path, &#34;results.csv&#34;))</code></pre>
</details>
</dd>
<dt id="app.formatting.combine_data"><code class="name flex">
<span>def <span class="ident">combine_data</span></span>(<span>data: list, name: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Combines data from all runs.
:return: None</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>list: </dd>
<dt><strong><code>name</code></strong></dt>
<dd>str: </dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_data(data: list, name: str) -&gt; None:
    &#34;&#34;&#34;Combines data from all runs.
    :return: None

    Args:
      data: list: 
      name: str: 

    Returns:

    &#34;&#34;&#34;
    data_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;filtered&#34;)
    tmp = list()
    for i, file in enumerate(data):
        tmp_data = pd.read_csv(filepath_or_buffer=os.path.join(data_path, f&#34;{file}.csv&#34;), delimiter=&#39;,&#39;)
        tmp_data.insert(0, &#39;run&#39;, i + 1)
        tmp_data = tmp_data.loc[(tmp_data[&#39;pod&#39;] == &#34;webui&#34;)]
        tmp.append(tmp_data)
    result = pd.concat(tmp)
    save_data(result, name, &#34;combined&#34;)</code></pre>
</details>
</dd>
<dt id="app.formatting.combine_runs"><code class="name flex">
<span>def <span class="ident">combine_runs</span></span>(<span>) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Combines data from all runs.
:return: None</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_runs() -&gt; None:
    &#34;&#34;&#34;Combines data from all runs.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    data = get_directories()
    data_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;filtered&#34;)
    tmp = list()
    for i, file in enumerate(data):
        tmp_data = pd.read_csv(filepath_or_buffer=os.path.join(data_path, f&#34;{file}.csv&#34;), delimiter=&#39;,&#39;)
        tmp_data.insert(0, &#39;run&#39;, i + 1)
        tmp_data = tmp_data.loc[(tmp_data[&#39;pod&#39;] == &#34;webui&#34;)]
        tmp.append(tmp_data)
    result = pd.concat(tmp)
    save_data(result, os.getenv(&#34;LAST_DATA&#34;), &#34;combined&#34;)</code></pre>
</details>
</dd>
<dt id="app.formatting.correlation_coefficient_matrix"><code class="name flex">
<span>def <span class="ident">correlation_coefficient_matrix</span></span>(<span>) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates and plots the correlation coefficient matrix for a given dataframe.
:return: None</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def correlation_coefficient_matrix() -&gt; None:
    &#34;&#34;&#34;Calculates and plots the correlation coefficient matrix for a given dataframe.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    dir_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;correlation&#34;, os.getenv(&#34;LAST_DATA&#34;))
    if not os.path.exists(dir_path):
        os.mkdir(dir_path)
    df = pd.read_csv(os.path.join(os.getcwd(), &#34;data&#34;, &#34;combined&#34;, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}.csv&#34;))
    df = df[
        [&#34;cpu limit&#34;, &#34;memory limit&#34;, &#34;number of pods&#34;, &#34;ratio response time&#34;, &#34;ratio cpu usage&#34;,
         &#34;ratio memory usage&#34;]]
    df.dropna()
    corr = df.corr(method=&#34;pearson&#34;)
    save_data(corr, os.getenv(&#34;LAST_DATA&#34;), os.path.join(&#34;correlation&#34;, os.getenv(&#34;LAST_DATA&#34;)))
    # plot correlation
    # Generate a mask for the upper triangle
    mask = np.triu(np.ones_like(corr, dtype=bool))
    # Set up the matplotlib figure
    f, ax = plt.subplots(figsize=(11, 9))
    # Generate a custom diverging colormap
    cmap = sns.color_palette(&#34;coolwarm&#34;, as_cmap=True)
    # Draw the heatmap with the mask and correct aspect ratio
    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0, square=True, linewidths=.5,
                cbar_kws={&#34;shrink&#34;: .5},
                annot=True, fmt=&#34;.2f&#34;)
    f.savefig(os.path.join(dir_path, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}.png&#34;), bbox_inches=&#39;tight&#39;)
    plt.show()</code></pre>
</details>
</dd>
<dt id="app.formatting.filter_all_data"><code class="name flex">
<span>def <span class="ident">filter_all_data</span></span>(<span>) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Filters all data between two dates.
:return: None</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_all_data() -&gt; None:
    &#34;&#34;&#34;Filters all data between two dates.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    # init
    i = 1
    dirs = get_directories()
    for d in dirs:
        # filter data in directory
        logging.info(f&#34;Filtering data: {d} {i}/{len(dirs)}&#34;)
        filter_data(d)
        i = i + 1</code></pre>
</details>
</dd>
<dt id="app.formatting.filter_data"><code class="name flex">
<span>def <span class="ident">filter_data</span></span>(<span>directory: str) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Filters data from prometheus.
:return: filtered data</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>directory</code></strong></dt>
<dd>str: </dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_data(directory: str) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Filters data from prometheus.
    :return: filtered data

    Args:
      directory: str: 

    Returns:

    &#34;&#34;&#34;
    normal, custom, locust = get_data(directory)
    # filter for namespace
    filtered_data = pd.concat(objs=[normal[normal.namespace.eq(os.getenv(&#34;NAMESPACE&#34;))]])
    # read variation matrices
    variation = get_variation_matrix(directory)
    # filter for pod name
    filtered_data[&#34;pod&#34;] = filtered_data[&#34;pod&#34;].str.split(&#34;-&#34;, n=2).str[1]
    custom[&#34;pod&#34;] = custom[&#34;pod&#34;].str.split(&#34;-&#34;, n=2).str[1]
    # count data points per iteration
    filtered_data[&#39;datapoint&#39;] = filtered_data.groupby([&#34;Iteration&#34;]).cumcount() + 1
    custom[&#39;datapoint&#39;] = custom.groupby([&#34;Iteration&#34;]).cumcount() + 1
    custom[&#39;pod&#39;].fillna(&#34;webui&#34;, inplace=True)
    # create pivot tables
    filtered_data = pd.pivot_table(filtered_data, index=[&#34;Iteration&#34;, &#34;pod&#34;, &#34;datapoint&#34;], columns=[&#34;__name__&#34;],
                                   values=&#34;value&#34;).reset_index()
    filtered_custom_data = pd.pivot_table(custom, index=[&#34;Iteration&#34;, &#34;pod&#34;, &#34;datapoint&#34;], columns=[&#34;metric&#34;],
                                          values=&#34;value&#34;).reset_index()
    # calculate median values
    filtered_data = filtered_data.groupby([&#34;Iteration&#34;, &#34;pod&#34;]).mean().reset_index()
    filtered_custom_data = filtered_custom_data.groupby([&#34;Iteration&#34;, &#34;pod&#34;]).mean().reset_index()
    filtered_custom_data.rename(columns={&#34;rps&#34;: &#34;average rps&#34;}, inplace=True)
    # outliers
    filtered_custom_data[&#34;median_latency&#34;] = np.where(
        filtered_custom_data[&#34;median_latency&#34;] &lt; filtered_custom_data[&#34;median_latency&#34;].quantile(0.10),
        filtered_custom_data[&#34;median_latency&#34;].quantile(0.10),
        filtered_custom_data[&#39;median_latency&#39;])
    filtered_custom_data[&#34;median_latency&#34;] = np.where(
        filtered_custom_data[&#34;median_latency&#34;] &gt; filtered_custom_data[&#34;median_latency&#34;].quantile(0.90),
        filtered_custom_data[&#34;median_latency&#34;].quantile(0.90),
        filtered_custom_data[&#39;median_latency&#39;])
    # merge all tables
    res_data = pd.merge(filtered_data, filtered_custom_data, how=&#39;left&#39;, on=[&#34;Iteration&#34;, &#34;pod&#34;])
    res_data = pd.merge(res_data, variation, how=&#39;left&#39;, on=[&#34;Iteration&#34;, &#34;pod&#34;])
    # erase stuff
    res_data.drop(columns=[&#34;kube_deployment_spec_replicas&#34;, &#34;kube_pod_container_resource_limits_cpu_cores&#34;,
                           &#34;kube_pod_container_resource_limits_memory_bytes&#34;,
                           &#34;kube_pod_container_resource_requests_cpu_cores&#34;,
                           &#34;kube_pod_container_resource_requests_memory_bytes&#34;,
                           &#34;datapoint_x&#34;, &#34;datapoint_y&#34;], inplace=True)
    res_data.rename(
        columns={&#34;cpu&#34;: &#34;cpu usage&#34;, &#34;memory&#34;: &#34;memory usage&#34;, &#34;CPU&#34;: &#34;cpu limit&#34;, &#34;Memory&#34;: &#34;memory limit&#34;,
                 &#34;Pods&#34;: &#34;number of pods&#34;, &#34;container_cpu_cfs_throttled_seconds_total&#34;: &#34;cpu throttled total&#34;,
                 &#34;response_time&#34;: &#34;average response time&#34;, &#34;median_latency&#34;: &#34;median latency&#34;},
        inplace=True)
    # ratio
    res_data[&#34;rps delta&#34;] = (res_data[&#34;average rps&#34;] - res_data[&#34;RPS&#34;]) / res_data[&#34;RPS&#34;]
    res_data[&#34;ratio response time&#34;] = res_data[&#34;median latency&#34;] * (1 - res_data[&#34;rps delta&#34;])
    res_data[&#34;ratio cpu usage&#34;] = res_data[&#34;cpu usage&#34;] * (1 - res_data[&#34;rps delta&#34;])
    res_data[&#34;ratio memory usage&#34;] = res_data[&#34;memory usage&#34;] * (1 - res_data[&#34;rps delta&#34;])
    # filter for webui pod
    res_data = res_data.loc[(res_data[&#39;pod&#39;] == &#34;webui&#34;)]
    res_data.reset_index(inplace=True)
    res_data.drop(columns=[&#34;index&#34;], inplace=True)
    # save
    save_data(res_data, directory, &#34;filtered&#34;)
    return res_data</code></pre>
</details>
</dd>
<dt id="app.formatting.filter_run"><code class="name flex">
<span>def <span class="ident">filter_run</span></span>(<span>) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Filters all data from a run.
:return: None</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_run() -&gt; None:
    &#34;&#34;&#34;Filters all data from a run.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;combined&#34;, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}.csv&#34;)
    data = pd.read_csv(path, delimiter=&#34;,&#34;)
    data = data.groupby([&#34;Iteration&#34;, &#34;pod&#34;]).median()
    save_data(data, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}_median&#34;, &#34;combined&#34;)</code></pre>
</details>
</dd>
<dt id="app.formatting.format_for_extra_p"><code class="name flex">
<span>def <span class="ident">format_for_extra_p</span></span>(<span>) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Formats a given benchmark for extra-p.
:return: None</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def format_for_extra_p() -&gt; None:
    &#34;&#34;&#34;Formats a given benchmark for extra-p.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    # init
    load_dotenv()
    save_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;formatted&#34;, os.getenv(&#34;LAST_DATA&#34;))
    # create directory if not existing
    if not os.path.exists(save_path):
        os.mkdir(save_path)
    # get variation matrix
    variation = get_variation_matrix(os.getenv(&#34;LAST_DATA&#34;))
    m_max = variation[&#34;Memory&#34;].nunique()
    # parameter and metrics
    parameter = [&#34;cpu limit&#34;, &#34;memory limit&#34;, &#34;number of pods&#34;, &#34;rps&#34;]
    targets = [&#34;average response time&#34;, &#34;cpu usage&#34;, &#34;memory usage&#34;]
    # get all filtered data
    filtered_data = list()
    for f in get_all_filtered_data():
        filtered_data.append(f)
    # write in txt file
    for metric in targets:
        m_name = (re.sub(&#39;[^a-zA-Z0-9 _]&#39;, &#39;&#39;, metric)).rstrip().replace(&#39; &#39;, &#39;_&#39;).lower()
        with open(os.path.join(save_path, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}_{m_name}_extra-p.txt&#34;), &#34;x&#34;) as file:
            # write parameters
            for par in parameter:
                file.write(f&#34;PARAMETER {(re.sub(&#39;[^a-zA-Z0-9 _]&#39;, &#39;&#39;, par)).rstrip().replace(&#39; &#39;, &#39;_&#39;).lower()}\n&#34;)
            file.write(&#34;\n&#34;)
            # write coordinates
            # for every iteration
            for i, v in enumerate(variation.index):
                if i % m_max == 0:
                    file.write(&#34;\n&#34;)
                    file.write(&#34;POINTS &#34;)
                row = variation.iloc[[i]]
                file.write(
                    f&#34;( {row.iloc[0][&#39;CPU&#39;]} {row.iloc[0][&#39;Memory&#39;]} {row.iloc[0][&#39;Pods&#39;]} {row.iloc[0][&#39;RPS&#39;]} ) &#34;)
            file.write(&#34;\n&#34;)
            file.write(&#34;\n&#34;)
            file.write(f&#34;REGION {os.getenv(&#39;APP_NAME&#39;)}\n&#34;)
            file.write(f&#34;METRIC {m_name}\n&#34;)
            file.write(&#34;\n&#34;)
            # write data
            # for every datapoint
            for i in range(0, (filtered_data[0].index.max() + 1)):
                logging.info(f&#34;format data: {i + 1}/{(filtered_data[0].index.max() + 1)}&#34;)
                file.write(&#34;DATA &#34;)
                # for test purposes
                for f in filtered_data:
                    x = f.loc[f.index == i, metric].iloc[0]
                    file.write(f&#34;{x} &#34;)
                file.write(&#34;\n&#34;)</code></pre>
</details>
</dd>
<dt id="app.formatting.formatting_evaluation"><code class="name flex">
<span>def <span class="ident">formatting_evaluation</span></span>(<span>date: str) ‑> (<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>)</span>
</code></dt>
<dd>
<div class="desc"><p>Formats evaluation data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>date</code></strong></dt>
<dd>date of evaluation raw folder</dd>
<dt><strong><code>date</code></strong></dt>
<dd>str) -&gt; (pd.DataFrame: </dd>
</dl>
<p>pd.DataFrame: </p>
<h2 id="returns">Returns</h2>
<p>custom and normal filtered data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def formatting_evaluation(date: str) -&gt; (pd.DataFrame, pd.DataFrame):
    &#34;&#34;&#34;Formats evaluation data.

    Args:
      date: date of evaluation raw folder
      date: str) -&gt; (pd.DataFrame: 
      pd.DataFrame: 

    Returns:
      custom and normal filtered data

    &#34;&#34;&#34;
    # get data
    dir_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;raw&#34;, f&#34;{date}&#34;)
    custom = pd.read_csv(os.path.join(dir_path, &#34;custom_metrics_0.csv&#34;), delimiter=&#39;,&#39;)
    normal = pd.read_csv(os.path.join(dir_path, &#34;metrics_0.csv&#34;), delimiter=&#39;,&#39;)
    if os.path.exists(os.path.join(dir_path, &#34;custom_metrics_0.csv&#34;)) and os.path.exists(
            os.path.join(dir_path, &#34;metrics_0.csv&#34;)):
        # filter for namespace
        filtered_data = pd.concat(objs=[normal[normal.namespace.eq(os.getenv(&#34;NAMESPACE&#34;))]])
        # filter for pod name
        filtered_data[&#34;pod&#34;] = filtered_data[&#34;pod&#34;].str.split(&#34;-&#34;, n=2).str[1]
        custom[&#34;pod&#34;] = custom[&#34;pod&#34;].str.split(&#34;-&#34;, n=2).str[1]
        custom[&#39;pod&#39;].fillna(&#34;webui&#34;, inplace=True)
        # timestamps in seconds
        filtered_data[&#34;timestamp&#34;] = pd.to_datetime(filtered_data[&#34;timestamp&#34;], unit=&#39;s&#39;, origin=&#39;unix&#39;)
        filtered_data[&#34;timestamp&#34;] = (filtered_data[&#34;timestamp&#34;] - filtered_data[&#34;timestamp&#34;].min())
        custom[&#34;timestamp&#34;] = pd.to_datetime(custom[&#34;timestamp&#34;], unit=&#39;s&#39;, origin=&#39;unix&#39;)
        custom[&#34;timestamp&#34;] = (custom[&#34;timestamp&#34;] - custom[&#34;timestamp&#34;].min())
        filtered_data[&#34;timestamp&#34;] = (filtered_data.timestamp / np.timedelta64(1, &#39;m&#39;))
        custom[&#34;timestamp&#34;] = (custom.timestamp / np.timedelta64(1, &#39;m&#39;))
        # filter for pod
        filtered_data_a = filtered_data[filtered_data[&#39;deployment&#39;] == &#34;teastore-webui&#34;]
        filtered_data_b = filtered_data[filtered_data[&#39;pod&#39;] == &#34;webui&#34;]
        filtered_data = pd.concat([filtered_data_a, filtered_data_b])
        custom = custom[(custom[&#39;pod&#39;] == &#34;webui&#34;)]
        # pivot
        filtered_data = pd.pivot_table(filtered_data, index=[&#34;timestamp&#34;], columns=[&#34;__name__&#34;],
                                       values=&#34;value&#34;).reset_index()
        filtered_custom_data = pd.pivot_table(custom, index=[&#34;timestamp&#34;], columns=[&#34;metric&#34;],
                                              values=&#34;value&#34;).reset_index()
        filtered_custom_data.set_index(&#34;timestamp&#34;)
        # fix units
        filtered_data[&#34;kube_pod_container_resource_limits_cpu_cores&#34;] = filtered_data[
                                                                            &#34;kube_pod_container_resource_limits_cpu_cores&#34;] * 1000
        filtered_data[&#34;kube_pod_container_resource_limits_memory_bytes&#34;] = filtered_data[
                                                                               &#34;kube_pod_container_resource_limits_memory_bytes&#34;] / 1048576
        filtered_data.to_csv(os.path.join(dir_path, &#34;filtered.csv&#34;))
        filtered_custom_data.to_csv(os.path.join(dir_path, &#34;filtered_custom.csv&#34;))
        return filtered_data, filtered_custom_data, dir_path
    else:
        logging.warning(f&#34;Metric files do not exist in folder: {date}&#34;)
        return</code></pre>
</details>
</dd>
<dt id="app.formatting.get_all_data"><code class="name flex">
<span>def <span class="ident">get_all_data</span></span>(<span>) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>Gets all metric tables between two dates.
:return: list of metric data</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_all_data() -&gt; list:
    &#34;&#34;&#34;Gets all metric tables between two dates.
    :return: list of metric data

    Args:

    Returns:

    &#34;&#34;&#34;
    # init
    all_data = list()
    for d in get_directories():
        p_data, c_data, l_data = get_data(d)
        # append to list
        all_data.append([p_data, c_data, l_data])
    return all_data</code></pre>
</details>
</dd>
<dt id="app.formatting.get_all_filtered_data"><code class="name flex">
<span>def <span class="ident">get_all_filtered_data</span></span>(<span>) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>Reads all filtered data between two dates.
:return: list of filtered data</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_all_filtered_data() -&gt; list:
    &#34;&#34;&#34;Reads all filtered data between two dates.
    :return: list of filtered data

    Args:

    Returns:

    &#34;&#34;&#34;
    load_dotenv()
    first_date = int(str(os.getenv(&#34;FIRST_DATA&#34;)).replace(&#39;-&#39;, &#34;&#34;).strip())
    last_date = int(str(os.getenv(&#34;LAST_DATA&#34;)).replace(&#39;-&#39;, &#34;&#34;).strip())
    base_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;filtered&#34;)
    files = list()
    # get data from each run
    for (dir_path, dir_names, filenames) in os.walk(base_path):
        for c_file in filenames:
            if str(c_file).endswith(&#34;.csv&#34;):
                c_date = int(str(c_file).replace(&#39;-&#39;, &#34;&#34;).replace(&#34;.csv&#34;, &#34;&#34;).strip())
                if last_date &gt;= c_date &gt;= first_date:
                    files.append(pd.read_csv(os.path.join(base_path, c_file)))
    return files</code></pre>
</details>
</dd>
<dt id="app.formatting.get_combined_data"><code class="name flex">
<span>def <span class="ident">get_combined_data</span></span>(<span>) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Args:</p>
<h2 id="returns">Returns</h2>
<p>:return: combined data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_combined_data() -&gt; pd.DataFrame:
    &#34;&#34;&#34;

    Args:

    Returns:
      :return: combined data

    &#34;&#34;&#34;
    # get last combined run
    path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;combined&#34;, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}.csv&#34;)
    data = pd.read_csv(path, delimiter=&#34;,&#34;)
    return data</code></pre>
</details>
</dd>
<dt id="app.formatting.get_data"><code class="name flex">
<span>def <span class="ident">get_data</span></span>(<span>directory: str) ‑> (<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.frame.DataFrame'>)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets data from prometheus.
:return: prometheus data</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>directory</code></strong></dt>
<dd>str) -&gt; (pd.DataFrame: </dd>
</dl>
<p>pd.DataFrame:
Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_data(directory: str) -&gt; (pd.DataFrame, pd.DataFrame, pd.DataFrame):
    &#34;&#34;&#34;Gets data from prometheus.
    :return: prometheus data

    Args:
      directory: str) -&gt; (pd.DataFrame: 
      pd.DataFrame: 

    Returns:

    &#34;&#34;&#34;
    # config
    load_dotenv(override=True)
    prometheus_data = None
    prometheus_custom_data = None
    locust_data = None
    # check if folder exists
    data_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;raw&#34;, directory)
    if os.path.exists(data_path):
        # search for prometheus metric files
        logging.info(f&#34;Gets data from {directory}.&#34;)
        for (dir_path, dir_names, filenames) in os.walk(data_path):
            for file in filenames:
                if &#34;metrics&#34; in file and &#34;custom_metrics&#34; not in file:
                    i = int(str(file).split(&#34;_&#34;)[1].rstrip(&#34;.csv&#34;))
                    prometheus_data = get_data_helper(prometheus_data, file, i, directory)
                elif &#34;custom_metrics&#34; in file:
                    j = int(str(file).split(&#34;_&#34;)[2].rstrip(&#34;.csv&#34;))
                    print(j)
                    prometheus_custom_data = get_data_helper(prometheus_custom_data, file, j, directory)
                elif &#34;locust&#34; in file and &#34;stats&#34; in file and &#34;history&#34; not in file:
                    l = int(str(file).split(&#34;_&#34;)[2].rstrip(&#34;.csv&#34;))
                    locust_data = get_data_helper(locust_data, file, l, directory)
    return prometheus_data, prometheus_custom_data, locust_data</code></pre>
</details>
</dd>
<dt id="app.formatting.get_data_helper"><code class="name flex">
<span>def <span class="ident">get_data_helper</span></span>(<span>data: pandas.core.frame.DataFrame, file: str, iteration: int, directory: str) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Connects two dataframes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>given dataframe</dd>
<dt><strong><code>file</code></strong></dt>
<dd>data frame in file</dd>
<dt><strong><code>iteration</code></strong></dt>
<dd>number of iteration</dd>
<dt><strong><code>directory</code></strong></dt>
<dd>date</dd>
<dt><strong><code>data</code></strong></dt>
<dd>pd.DataFrame: </dd>
<dt><strong><code>file</code></strong></dt>
<dd>str: </dd>
<dt><strong><code>iteration</code></strong></dt>
<dd>int: </dd>
<dt><strong><code>directory</code></strong></dt>
<dd>str: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>connected data frame</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_data_helper(data: pd.DataFrame, file: str, iteration: int, directory: str) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Connects two dataframes.

    Args:
      data: given dataframe
      file: data frame in file
      iteration: number of iteration
      directory: date
      data: pd.DataFrame: 
      file: str: 
      iteration: int: 
      directory: str: 

    Returns:
      connected data frame

    &#34;&#34;&#34;
    data_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;raw&#34;, directory)
    # concat metrics
    tmp_data = pd.read_csv(filepath_or_buffer=os.path.join(data_path, file), delimiter=&#39;,&#39;)
    tmp_data.insert(0, &#39;Iteration&#39;, iteration)
    if data is None:
        data = tmp_data
    else:
        data = pd.concat([data, tmp_data])
    return data</code></pre>
</details>
</dd>
<dt id="app.formatting.get_directories"><code class="name flex">
<span>def <span class="ident">get_directories</span></span>(<span>) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>Gets all directory names between the first and last data date.
:return: list of directory names</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_directories() -&gt; list:
    &#34;&#34;&#34;Gets all directory names between the first and last data date.
    :return: list of directory names

    Args:

    Returns:

    &#34;&#34;&#34;
    load_dotenv()
    first_date = int(str(os.getenv(&#34;FIRST_DATA&#34;)).replace(&#39;-&#39;, &#34;&#34;).strip())
    last_date = int(str(os.getenv(&#34;LAST_DATA&#34;)).replace(&#39;-&#39;, &#34;&#34;).strip())
    base_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;raw&#34;)
    dirs = list()
    # get data from each run
    for (dir_path, dir_names, filenames) in os.walk(base_path):
        for c_dir in dir_names:
            if &#34;dataset&#34; not in c_dir:
                c_date = int(str(c_dir).replace(&#39;-&#39;, &#34;&#34;).strip())
                if last_date &gt;= c_date &gt;= first_date:
                    dirs.append(c_dir)
    return dirs</code></pre>
</details>
</dd>
<dt id="app.formatting.get_filtered_data"><code class="name flex">
<span>def <span class="ident">get_filtered_data</span></span>(<span>directory: str) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a data frame of a given filtered data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>directory</code></strong></dt>
<dd>date</dd>
<dt><strong><code>directory</code></strong></dt>
<dd>str: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>data frame of filtered data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_filtered_data(directory: str) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Returns a data frame of a given filtered data.

    Args:
      directory: date
      directory: str: 

    Returns:
      data frame of filtered data

    &#34;&#34;&#34;
    base_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;filtered&#34;)
    for (dir_path, dir_names, filenames) in os.walk(base_path):
        for c_file in filenames:
            if directory in c_file:
                df = pd.read_csv(os.path.join(base_path, c_file))
                return df</code></pre>
</details>
</dd>
<dt id="app.formatting.get_variation_matrix"><code class="name flex">
<span>def <span class="ident">get_variation_matrix</span></span>(<span>directory: str) ‑> <built-in function array></span>
</code></dt>
<dd>
<div class="desc"><p>Reads all variation matrices of a directory and puts them in a list.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>directory</code></strong></dt>
<dd>current directory</dd>
<dt><strong><code>directory</code></strong></dt>
<dd>str: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>variation matrix</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_variation_matrix(directory: str) -&gt; np.array:
    &#34;&#34;&#34;Reads all variation matrices of a directory and puts them in a list.

    Args:
      directory: current directory
      directory: str: 

    Returns:
      variation matrix

    &#34;&#34;&#34;
    dir_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;raw&#34;, directory)
    # find variation files
    for (dir_path, dir_names, filenames) in os.walk(dir_path):
        for file in filenames:
            if &#34;variation&#34; in file:
                # filter name
                name = str(file).split(&#34;-&#34;)[1].split(&#34;_&#34;)[0]
                # read variation file
                file_path = os.path.join(dir_path, file)
                res = pd.read_csv(filepath_or_buffer=file_path, delimiter=&#39;,&#39;)
                # edit table
                res.insert(0, &#39;pod&#39;, name)
                res.rename(columns={&#34;Unnamed: 0&#34;: &#34;Iteration&#34;}, inplace=True)
                res.reset_index()
                return res</code></pre>
</details>
</dd>
<dt id="app.formatting.histogram"><code class="name flex">
<span>def <span class="ident">histogram</span></span>(<span>) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Creates histogram of the average response time.
:return: None</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def histogram() -&gt; None:
    &#34;&#34;&#34;Creates histogram of the average response time.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    sns.set_style(&#34;whitegrid&#34;)
    data = get_combined_data()
    ax = sns.histplot(data, x=&#34;average response time&#34;,
                      bins=50, binrange=(250, 5000))
    ax.set_xlabel(&#34;Average Response Time [ms]&#34;)
    plt.show()</code></pre>
</details>
</dd>
<dt id="app.formatting.plot_all_data"><code class="name flex">
<span>def <span class="ident">plot_all_data</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Plots data from all runs.
:return: None</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_all_data():
    &#34;&#34;&#34;Plots data from all runs.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    directory = os.path.join(os.getcwd(), &#34;data&#34;, &#34;plots&#34;, os.getenv(&#34;LAST_DATA&#34;))
    # creates folder if does not exist
    if not os.path.exists(directory):
        os.mkdir(directory)
    for i, file in enumerate(get_all_filtered_data()):
        plot_filtered_data(file, os.path.join(os.getenv(&#34;LAST_DATA&#34;), str(i)))</code></pre>
</details>
</dd>
<dt id="app.formatting.plot_all_evaluation"><code class="name flex">
<span>def <span class="ident">plot_all_evaluation</span></span>(<span>) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Plots all evaluations in the raw folder.
:return: None</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_all_evaluation() -&gt; None:
    &#34;&#34;&#34;Plots all evaluations in the raw folder.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    raw_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;raw&#34;)
    for (dir_path, dir_names, filenames) in os.walk(raw_path):
        for d in dir_names:
            plot_evaluation(d)</code></pre>
</details>
</dd>
<dt id="app.formatting.plot_evaluation"><code class="name flex">
<span>def <span class="ident">plot_evaluation</span></span>(<span>date: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Plots a given evaluation run.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>date</code></strong></dt>
<dd>name of folder</dd>
<dt><strong><code>date</code></strong></dt>
<dd>str: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_evaluation(date: str) -&gt; None:
    &#34;&#34;&#34;Plots a given evaluation run.

    Args:
      date: name of folder
      date: str: 

    Returns:
      None

    &#34;&#34;&#34;
    # format raw data
    n, c, dir_path = formatting_evaluation(date)

    # kube metrics
    nfig, (nax1, nax2, nax3) = plt.subplots(nrows=3, ncols=1)
    # cpu limit
    sns.lineplot(data=n, x=&#34;timestamp&#34;, y=&#34;kube_pod_container_resource_limits_cpu_cores&#34;, ax=nax1)
    nax1.set_ylabel(&#34;CPU limit [m]&#34;)
    # memory limit
    sns.lineplot(data=n, x=&#34;timestamp&#34;, y=&#34;kube_pod_container_resource_limits_memory_bytes&#34;, ax=nax2)
    nax2.set_ylabel(&#34;Memory limit [Mi]&#34;)
    # number of pods
    sns.lineplot(data=n, x=&#34;timestamp&#34;, y=&#34;kube_deployment_spec_replicas&#34;, ax=nax3)
    nax3.set_ylabel(&#34;Number of pods&#34;)
    nfig.savefig(os.path.join(dir_path, &#34;parameters.png&#34;))
    nfig.clf()

    # custom metrics
    cfig, (cax1, cax2, cax3) = plt.subplots(nrows=3, ncols=1)
    # average response time
    sns.lineplot(data=c, x=&#34;timestamp&#34;, y=&#34;response_time&#34;, ax=cax1)
    cax1.set_ylabel(&#34;Response time [ms]&#34;)
    # cpu
    sns.lineplot(data=c, x=&#34;timestamp&#34;, y=&#34;cpu&#34;, ax=cax2)
    cax2.set_ylabel(&#34;CPU usage [%]&#34;)
    # memory
    sns.lineplot(data=c, x=&#34;timestamp&#34;, y=&#34;memory&#34;, ax=cax3)
    cax3.set_ylabel(&#34;Memory usage [%]&#34;)
    cfig.savefig(os.path.join(dir_path, &#34;targets.png&#34;))
    cfig.clf()

    description = {&#34;response_time&#34;: &#34;Average response time [ms]&#34;, &#34;cpu&#34;: &#34;CPU usage [%]&#34;, &#34;memory&#34;: &#34;Memory usage [%]&#34;}
    for t in [&#34;response_time&#34;, &#34;cpu&#34;, &#34;memory&#34;]:
        ax = sns.histplot(c, x=t)
        ax.set_xlabel(description[t])
        fig = ax.get_figure()
        fig.savefig(os.path.join(dir_path, f&#34;{t}_histogram.png&#34;))
        fig.clf()
    # rps
    ax = sns.lineplot(data=c, x=&#34;timestamp&#34;, y=&#34;rps&#34;)
    ax.set_xlabel(&#34;Minutes&#34;)
    ax.set_ylabel(&#34;Requests per second&#34;)
    ax.figure.savefig(os.path.join(dir_path, &#34;rps.png&#34;))
    cfig.clf()
    calc_eval_metrics(c, n, dir_path)</code></pre>
</details>
</dd>
<dt id="app.formatting.plot_filtered_data"><code class="name flex">
<span>def <span class="ident">plot_filtered_data</span></span>(<span>data: pandas.core.frame.DataFrame, name: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Plots a given metric from filtered data from prometheus.
:return: None</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>pd.DataFrame: </dd>
<dt><strong><code>name</code></strong></dt>
<dd>str: </dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_filtered_data(data: pd.DataFrame, name: str) -&gt; None:
    &#34;&#34;&#34;Plots a given metric from filtered data from prometheus.
    :return: None

    Args:
      data: pd.DataFrame: 
      name: str: 

    Returns:

    &#34;&#34;&#34;
    # create directory
    dir_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;plots&#34;, f&#34;{name}&#34;)
    os.mkdir(dir_path)
    # init x- and y-axis
    x_axis = [&#34;cpu limit&#34;, &#34;memory limit&#34;, &#34;number of pods&#34;]
    y_axis = [&#34;ratio response time&#34;, &#34;ratio cpu usage&#34;,
              &#34;ratio memory usage&#34;]
    # functions
    functions = [pd.DataFrame.min, pd.DataFrame.median, pd.DataFrame.max]
    # create and save plots
    plot = None
    for fn in functions:
        for y in y_axis:
            for x in x_axis:
                if x == &#34;number of pods&#34;:
                    data_pods = data.loc[(data[&#39;memory limit&#39;] == fn(data[&#39;memory limit&#39;])) &amp; (
                            data[&#39;cpu limit&#39;] == fn(data[&#39;cpu limit&#39;]))]
                    plot = sns.lineplot(data=data_pods, x=x, y=y)
                elif x == &#34;memory limit&#34;:
                    data_memory = data.loc[(data[&#39;cpu limit&#39;] == fn(data[&#39;cpu limit&#39;]))]
                    plot = sns.lineplot(data=data_memory, x=x, y=y, hue=&#34;number of pods&#34;)
                elif x == &#34;cpu limit&#34;:
                    data_cpu = data.loc[(data[&#39;memory limit&#39;] == fn(data[&#39;memory limit&#39;]))]
                    plot = sns.lineplot(data=data_cpu, x=x, y=y, hue=&#34;number of pods&#34;)
                name = f&#34;{x}_{y}_{fn.__name__}.png&#34;
                plot.figure.savefig(os.path.join(dir_path, name))
                plot.figure.clf()
    # Requests per second
    fig, ax = plt.subplots()
    ax.plot(data[&#34;Iteration&#34;], data[&#34;average rps&#34;])
    ax.plot(data[&#34;Iteration&#34;], data[&#34;RPS&#34;])
    fig.savefig(os.path.join(dir_path, &#34;rps.png&#34;))
    fig.clf()</code></pre>
</details>
</dd>
<dt id="app.formatting.plot_run"><code class="name flex">
<span>def <span class="ident">plot_run</span></span>(<span>) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Plots all data from a run.
:return: None</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_run() -&gt; None:
    &#34;&#34;&#34;Plots all data from a run.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    # plot combined run
    path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;combined&#34;, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}.csv&#34;)
    data = pd.read_csv(path, delimiter=&#34;,&#34;)
    plot_filtered_data(data, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}_combined&#34;)
    # plot mean run
    path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;combined&#34;, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}_median.csv&#34;)
    data = pd.read_csv(path, delimiter=&#34;,&#34;)
    plot_filtered_data(data, f&#34;{os.getenv(&#39;LAST_DATA&#39;)}_combined_median&#34;)</code></pre>
</details>
</dd>
<dt id="app.formatting.plot_targets_4d"><code class="name flex">
<span>def <span class="ident">plot_targets_4d</span></span>(<span>data: pandas.core.frame.DataFrame, name: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Plot all parameters and each target in a 4D plot.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>dataset</dd>
<dt><strong><code>name</code></strong></dt>
<dd>save name</dd>
<dt><strong><code>data</code></strong></dt>
<dd>pd.DataFrame: </dd>
<dt><strong><code>name</code></strong></dt>
<dd>str: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_targets_4d(data: pd.DataFrame, name: str) -&gt; None:
    &#34;&#34;&#34;Plot all parameters and each target in a 4D plot.

    Args:
      data: dataset
      name: save name
      data: pd.DataFrame: 
      name: str: 

    Returns:
      None

    &#34;&#34;&#34;
    targets = [&#34;average response time&#34;, &#34;cpu usage&#34;, &#34;memory usage&#34;]
    for t in targets:
        fig = plt.figure()
        ax = fig.add_subplot(111, projection=&#39;3d&#39;)

        x = data[&#34;cpu limit&#34;]
        y = data[&#34;memory limit&#34;]
        z = data[&#34;number of pods&#34;]
        c = data[t]

        img = ax.scatter(x, y, z, c=c, cmap=plt.jet())
        fig.colorbar(img)
        plt.show()
        # save figure
        save_path = os.path.join(os.getcwd(), &#34;data&#34;, &#34;plots&#34;, name)
        if not os.path.exists(save_path):
            os.mkdir(save_path)
        fig.savefig(os.path.join(save_path, f&#34;{t}_3D.png&#34;))
        fig.clf()</code></pre>
</details>
</dd>
<dt id="app.formatting.process_all_runs"><code class="name flex">
<span>def <span class="ident">process_all_runs</span></span>(<span>) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Processes all runs between start- and last data.
:return: None</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_all_runs() -&gt; None:
    &#34;&#34;&#34;Processes all runs between start- and last data.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    filter_all_data()
    plot_all_data()
    combine_runs()
    filter_run()
    plot_run()
    format_for_extra_p()
    correlation_coefficient_matrix()</code></pre>
</details>
</dd>
<dt id="app.formatting.process_run"><code class="name flex">
<span>def <span class="ident">process_run</span></span>(<span>) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Processes one single run.
:return: None</p>
<p>Args:</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_run() -&gt; None:
    &#34;&#34;&#34;Processes one single run.
    :return: None

    Args:

    Returns:

    &#34;&#34;&#34;
    filter_data(os.getenv(&#34;LAST_DATA&#34;))
    plot_filtered_data(get_filtered_data(os.getenv(&#34;LAST_DATA&#34;)), os.getenv(&#34;LAST_DATA&#34;))</code></pre>
</details>
</dd>
<dt id="app.formatting.save_data"><code class="name flex">
<span>def <span class="ident">save_data</span></span>(<span>data: pandas.core.frame.DataFrame, directory: str, mode: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a given data frame in a given folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>data frame</dd>
<dt><strong><code>directory</code></strong></dt>
<dd>name of file</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>name of directory</dd>
<dt><strong><code>data</code></strong></dt>
<dd>pd.DataFrame: </dd>
<dt><strong><code>directory</code></strong></dt>
<dd>str: </dd>
<dt><strong><code>mode</code></strong></dt>
<dd>str: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_data(data: pd.DataFrame, directory: str, mode: str) -&gt; None:
    &#34;&#34;&#34;Saves a given data frame in a given folder.

    Args:
      data: data frame
      directory: name of file
      mode: name of directory
      data: pd.DataFrame: 
      directory: str: 
      mode: str: 

    Returns:
      None

    &#34;&#34;&#34;
    save_path = os.path.join(os.getcwd(), &#34;data&#34;, mode, f&#34;{directory}.csv&#34;)
    if not os.path.exists(save_path):
        data.to_csv(path_or_buf=save_path)
    else:
        logging.warning(&#34;Filtered data already exists.&#34;)</code></pre>
</details>
</dd>
<dt id="app.formatting.stats"><code class="name flex">
<span>def <span class="ident">stats</span></span>(<span>column: str) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Prints stats of a given column of the combined data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>column</code></strong></dt>
<dd>metric to be described</dd>
<dt><strong><code>column</code></strong></dt>
<dd>str: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stats(column: str) -&gt; None:
    &#34;&#34;&#34;Prints stats of a given column of the combined data.

    Args:
      column: metric to be described
      column: str: 

    Returns:
      None

    &#34;&#34;&#34;
    data = get_combined_data()
    print(data[column].describe())</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="app" href="index.html">app</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="app.formatting.boxplot" href="#app.formatting.boxplot">boxplot</a></code></li>
<li><code><a title="app.formatting.calc_eval_metrics" href="#app.formatting.calc_eval_metrics">calc_eval_metrics</a></code></li>
<li><code><a title="app.formatting.combine_data" href="#app.formatting.combine_data">combine_data</a></code></li>
<li><code><a title="app.formatting.combine_runs" href="#app.formatting.combine_runs">combine_runs</a></code></li>
<li><code><a title="app.formatting.correlation_coefficient_matrix" href="#app.formatting.correlation_coefficient_matrix">correlation_coefficient_matrix</a></code></li>
<li><code><a title="app.formatting.filter_all_data" href="#app.formatting.filter_all_data">filter_all_data</a></code></li>
<li><code><a title="app.formatting.filter_data" href="#app.formatting.filter_data">filter_data</a></code></li>
<li><code><a title="app.formatting.filter_run" href="#app.formatting.filter_run">filter_run</a></code></li>
<li><code><a title="app.formatting.format_for_extra_p" href="#app.formatting.format_for_extra_p">format_for_extra_p</a></code></li>
<li><code><a title="app.formatting.formatting_evaluation" href="#app.formatting.formatting_evaluation">formatting_evaluation</a></code></li>
<li><code><a title="app.formatting.get_all_data" href="#app.formatting.get_all_data">get_all_data</a></code></li>
<li><code><a title="app.formatting.get_all_filtered_data" href="#app.formatting.get_all_filtered_data">get_all_filtered_data</a></code></li>
<li><code><a title="app.formatting.get_combined_data" href="#app.formatting.get_combined_data">get_combined_data</a></code></li>
<li><code><a title="app.formatting.get_data" href="#app.formatting.get_data">get_data</a></code></li>
<li><code><a title="app.formatting.get_data_helper" href="#app.formatting.get_data_helper">get_data_helper</a></code></li>
<li><code><a title="app.formatting.get_directories" href="#app.formatting.get_directories">get_directories</a></code></li>
<li><code><a title="app.formatting.get_filtered_data" href="#app.formatting.get_filtered_data">get_filtered_data</a></code></li>
<li><code><a title="app.formatting.get_variation_matrix" href="#app.formatting.get_variation_matrix">get_variation_matrix</a></code></li>
<li><code><a title="app.formatting.histogram" href="#app.formatting.histogram">histogram</a></code></li>
<li><code><a title="app.formatting.plot_all_data" href="#app.formatting.plot_all_data">plot_all_data</a></code></li>
<li><code><a title="app.formatting.plot_all_evaluation" href="#app.formatting.plot_all_evaluation">plot_all_evaluation</a></code></li>
<li><code><a title="app.formatting.plot_evaluation" href="#app.formatting.plot_evaluation">plot_evaluation</a></code></li>
<li><code><a title="app.formatting.plot_filtered_data" href="#app.formatting.plot_filtered_data">plot_filtered_data</a></code></li>
<li><code><a title="app.formatting.plot_run" href="#app.formatting.plot_run">plot_run</a></code></li>
<li><code><a title="app.formatting.plot_targets_4d" href="#app.formatting.plot_targets_4d">plot_targets_4d</a></code></li>
<li><code><a title="app.formatting.process_all_runs" href="#app.formatting.process_all_runs">process_all_runs</a></code></li>
<li><code><a title="app.formatting.process_run" href="#app.formatting.process_run">process_run</a></code></li>
<li><code><a title="app.formatting.save_data" href="#app.formatting.save_data">save_data</a></code></li>
<li><code><a title="app.formatting.stats" href="#app.formatting.stats">stats</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>